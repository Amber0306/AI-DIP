# 0.目标检测loss简述

# 1.分类loss
## 1.1交叉熵损失CE

## 1.2 focal loss
### motivation
在one-stage检测算法中，会出现正负样本数量不平衡以及难易样本数量不平衡的情况，为了解决则以问题提出了focal loss。

hit的检测框就是正样本。容易的正样本是指置信度高且hit的检测框，困难的负样本就是置信度低但hit的检测框，容易的负样本是指未hit且置信度低的检测框，困难的负样本指未hit但置信度高的检测框。
### solution
目的是解决样本数量不平衡的情况 - 正样本loss增加，负样本loss减小 - 难样本loss增加，简单样本loss减小

# 2.回归loss

## 2.1 L1 loss
$$
loss(x,y) = \frac{1}{n} \sum_{i=1}^{n} \rvert y_i - f(x_i)\rvert
$$
- l1 loss在零点不平滑，用的较少。一般来说，l1正则会制造稀疏的特征，大部分无用的特征的权重会被置为0。

- （适合回归任务，简单的模型，由于神经网络通常解决复杂问题，很少使用。）

- L1 loss的鲁棒性(抗干扰性)比L2 loss强。概括起来就是L1对异常点不太敏感，而L2则会对异常点存在放大效果。因为L2将误差平方化，当误差大于1时，误会会放大很多，所以使用L2 loss的模型的误差会比使用L1 loss的模型对异常点更敏感。如果这个样本是一个异常值，模型就需要调整以适应单个的异常值，这会牺牲许多其它正常的样本，因为这些正常样本的误差比这单个的异常值的误差小。如果异常值对研究很重要，最小均方误差则是更好的选择。

- L1 loss 对 x(损失值)的导数为常数，在训练后期，x较小时，若学习率不变，损失函数会在稳定值附近波动，很难收敛到更高的精度。

- L2 loss的稳定性比L1 loss好。概括起来就是对于新数据的调整，L1的变动很大，而L2的则整体变动不大。

### L1不可导？
当损失函数不可导,梯度下降不再有效,可以使用坐标轴下降法。
梯度下降是沿着当前点的负梯度方向进行参数更新；
而坐标轴下降法是沿着坐标轴的方向；
假设有m个特征个数,坐标轴下降法进行参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。

使用Proximal Algorithm对L1进行求解,此方法是去优化损失函数上界结果。

## 2.2 L2 loss
$$
loss(x,y) = \frac{1}{n}\sum_{i=1}^{n}(y_i-f(x_i))^2
$$
- 对离群点比较敏感，如果feature是unbounded的话，需要好好调整学习率，防止出现梯度爆炸的情况。l2正则会让特征的权重不过大，使得特征的权重比较平均。

- （适合回归任务，数值特征不大，问题维度不高）

- 从L2 loss的图像可以看到，图像(上图左边红线)的每一点的导数都不一样的，离最低点越远，梯度越大，使用梯度下降法求解的时候梯度很大，可能导致梯度爆炸。

- L1 loss一般用于简单的模型，但由于神经网络一般是解决复杂的问题，所以很少用L1 loss，例如对于CNN网络，一般使用的是L2-loss，因为L2-loss的收敛速度比L1-loss要快。如下图：

## 2.3 Smooth L1 loss
$$
loss(x,y)=\frac{1}{n}\sum_{i=1}^n 
\begin{cases}
0.5 * (y_i-f(x_i))^2, & if\rvert y_i-f(x_i)\rvert <1 \\
\lvert y_i-f(x_i) \rvert - 0.5, & otherwise
\end{cases}
$$
- 修改零点不平滑问题，L1-smooth比l2 loss对异常值的鲁棒性更强。具有l1和l2的优点，当绝对差值小于1，梯度不至于太大，损失函数较平滑，当差别大的时候，梯度值足够小，较稳定，不容易梯度爆炸。

- （回归，当特征中有较大的数值，适合大多数问题）

- 当预测值和ground truth差别较小的时候（绝对值差小于1），其实使用的是L2 Loss，当绝对值差小于1时，由于L2会对误差进行平方，因此会得到更小的损失，有利于模型收敛。而当差别大的时候，是L1 Loss的平移，因此相比于L2损失函数，其对离群点（指的是距离中心较远的点）、异常值（outlier）不敏感，可控制梯度的量级使训练时不容易跑飞。。SooothL1Loss其实是L2Loss和L1Loss的结合，它同时拥有L2 Loss和L1 Loss的部分优点。

![smmoth l1](images/loss/1.png)

**Smooth L1**
当预测框与 ground truth 差别过大时，梯度值不至于过大；
当预测框与 ground truth 差别很小时，梯度值足够小。

## 2.4 IOU loss

## 2.5 GIOU loss

## 2.6 DIOU loss

## 2.7 CIOU loss