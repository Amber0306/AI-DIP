DetectoRS: Detecting Object with Recursive Feature Pyramid and Switchable Atrous Convolution

用递归特征金字塔和可切换的反卷积做目标检测

# Abstract

许多现代物体检测器通过利用观察和思考的机制表现杰出。在本文中，我们探讨了主干设计中的这种机制。在宏观层面上。我们提出了递归特征金字塔，它包含了额外的反馈连接，从特征金字塔网络到自下而上的骨干层中。
在微观层面上，我们提出了可切换的非线性卷积，它以不同的非线性速率对特征进行卷积，并使用切换函数来收集结果。将它们结合起来的结果是DetectoRS。
它极大地提高了物体检测的性能。
在COCO测试开发中，DetectoRS实现了最先进的55.7%的物体检测箱AP，48.5%的掩码AP用于例子分割的48.5%的掩膜AP，以及50.0%的全景分割的PQ。
该代码是公开的1。

# 1.Introduction

为了检测物体，人类视觉通过反馈连接传递高级语义信息，选择性地增强和抑制神经元激活[2,20,21]。受人类视觉系统的启发，观察和思考的机制在计算机视觉中得到了具体化，并表现出卓越的性能[5,6,62]。许多流行的两级物体探测器，例如。G更快的R-CNN[62]，首先输出对象建议，然后根据这些建议提取区域特征以检测对象。按照相同的方向，Cascade  R-CNN[5]开发了一种多级检测器，在该检测器中，后续的检测器头都经过了更具选择性的示例培训。这种设计理念的成功促使我们在目标检测的神经网络主干设计中探索它。特别是，我们在宏观和微观两个层面上部署了该机制，从而使我们提出的探测器大大提高了最先进的目标探测器HTC[8]的性能，而类似的推理速度保持不变，如选项卡所示。1.

在宏观层面上，我们提出的递归特征金字塔（RFP）建立在特征金字塔网络（FPN）[48]的基础上，将FPN层的额外反馈连接合并到自下而上的主干层中，如图1a所示。将递归结构展开为一个顺序实现，我们获得了一个对象检测器的主干，它可以查看图像两次或更多次。与级联R-CNN中的级联检测器头类似，我们的RFP递归地增强FPN，以生成越来越强大的表示。类似于深度监督网络[39]，反馈连接将直接从检测器头接收梯度的功能带回自底向上主干的低水平，以加快训练并提高性能。我们提出的RFP实现了一种顺序设计，即仔细观察和思考，其中自底向上主干网和FPN运行多次，其输出特性取决于前面步骤中的输出特性。

在微观层面，我们提出了可切换的萎缩卷积（SAC），它将相同的输入特征与不同的萎缩率卷积[12,32,57]，并使用切换函数收集结果。图1b示出了SAC概念的图示。开关函数是空间相关的，即。E特征图的每个位置可能有不同的开关来控制SAC的输出。为了在检测器中使用SAC，我们将自下而上主干中的所有标准3x3卷积层转换为SAC，这大大提高了检测器的性能。以前的一些方法采用条件卷积，例如。G[43,80]，它还将不同卷积的结果合并为单个输出。不同于那些体系结构需要从头开始训练，SAC提供了一种机制来轻松转换预训练的标准卷积网络（例如，ImageNet预训练的[63]检查点）。此外，在SAC中使用了一种新的重量锁定机制，其中除了可训练的差异外，不同萎缩卷积的重量是相同的。

在我们的检测器中结合提议的RFP和SAC结果。为了证明它的有效性，我们在具有挑战性的COCO数据集[51]上将探测器纳入最先进的HTC[8]。在COCOtest  dev上，我们报告了用于对象检测的框AP[23]，用于实例分割的掩码AP[28]，以及用于全景分割的PQ[37]。以ResNet-50[30]为主干的探测器将HTC[8]显著提高了7%。7%框AP和5。9%遮罩AP。此外，为我们的探测器配备ResNeXt-101-64x4d[77]实现了最先进的55。7%框AP和48。5%遮罩AP。加上DeepLabv3[15]以Wide-ResNet-41[11]为主干的物质预测，探测器创下了50的新纪录。0%PQ用于全景分割。

# 2. Related Work

## 目标检测

有两种主要的目标检测方法：一种是单阶段方法，即。G[40,49,54,60,64,73,86,87]和多阶段方法，如。G[5,7,8,10,26,27,  29,34,62,75].  多级探测器通常比单级探测器更灵活、更精确，但更复杂。在本文中，我们使用多级探测器HTC[8]作为基线，并与这两类探测器进行比较。多尺度特征。我们的递归特征金字塔基于特征金字塔网络（FPN）[48]，这是一种利用多尺度特征的有效目标检测系统。以前，许多对象检测器直接使用从主干提取的多尺度特征[4,54]，而FPN采用自上而下的路径来顺序组合不同尺度的特征。PANet[53]在FPN的顶部添加了另一条自底向上的路径。STDL[88]建议通过刻度传输模块利用交叉刻度特性。G-FRNet[1]使用选通单元添加反馈。NAS-FPN[25]和自动FPN[79]使用神经架构搜索[93]以找到最佳FPN结构。EfficientSet[70]建议重复一个简单的BiFPN层。与它们不同的是，我们提出的递归特征金字塔反复通过自下而上的主干，以丰富FPN的表示能力。此外，我们将Atrus  Spatial Pyramid Pooling（ASPP）[14,15]合并到FPN中，以丰富功能，类似于无缝[59]中的mini DeepLab设计。

## 递归卷积网络

许多递归方法被提出来解决不同类型的计算机视觉问题，例如。G[35,46,69].  最近，一种递归方法CBNet[55]被提出用于目标检测，该方法将多个主干级联以输出特征作为FPN的输入。相比之下，我们的RFP使用建议的ASPP丰富的FPN以及有效的融合模块执行递归计算。

## 条件卷积

条件卷积网络采用动态核、宽度或深度。G[17,43,47,52,80,83].  与它们不同的是，我们提出的可切换萎缩卷积（SAC）允许在不改变任何预训练模型的情况下，有效地将标准卷积转换为条件卷积。因此，SAC是许多预训练主干的即插即用模块。此外，SAC使用全局上下文信息和新的权重锁定机制使其更有效。

# 3.Recursive Feature Pyramid

## 3.1 Feature Pyramid Network

本小节介绍特征金字塔网络（FPN）的背景知识。让我们注意到自底向上主干的第四阶段，并注意到自顶向下FPN操作的第四阶段。配备FPN的主干网输出一组特征映射{fi  | i=1，…，S}，其中为阶段数。例如，图2a中的S＝3。∀i=1，S、 输出特性fi由定义

其中x0为输入图像，fs 1=0。基于FPN的目标检测器用于检测计算。

## 3.2 Recursive Feature Pyramid

我们对ResNet[30]backbonebt进行了更改，以允许它将两个xandr（f）作为其输入。ResNet有四个阶段，每个阶段由几个相似的块组成。如图3所示，我们仅对每个块的第一阶段进行更改。此块计算3层要素并将其添加到由快捷方式计算的要素中。为了使用特征R（f），我们添加了另一个卷积层，内核大小设置为1。该层的权重初始化为0，以确保当我们从预先训练的检查点加载权重时，它不会产生任何实际效果。

## 3.3 ASPP as the Connecting Module

我们使用Atrus  Spatial Pyramid  Pooling（ASPP）[13]来实现连接模块，该模块将特征作为输入，并将其转换为图3中使用的RFP特征。在这个模块中，有四个以fti为输入的并行分支，它们的输出沿着通道维度连接在一起，形成最终的输出ofR。它们的三个分支使用卷积层和ReLU层，输出通道数是输入通道数的1/4。最后一个分支使用全局平均池层压缩特征，然后使用1x1卷积层和ReLU层将压缩特征转换为a1/4大小（通道）特征。最后，它被调整大小并与来自其他三个分支的特征连接起来。这三个分支中的卷积层具有以下配置：内核大小=[1,3,3]，异步速率=[1,3,6]，填充=[0,3,6]。与最初的ASPP[13]不同，我们在串联特征之后没有卷积层，因为在这里没有生成密集预测任务中使用的最终输出。请注意，四个分支中的每一个都会生成一个通道1/4与输入特征相同的特征，将它们连接起来会生成一个与输入特征ofR大小相同的特征。以秒计。5，我们展示了RFP在有和没有ASPP模块的情况下的性能。

## 3.4 Output Update by the Fusion Module

如图2c所示，我们的RFP另外使用一个融合模块来组合DFT  1，以更新Equ中使用的展开阶段的OFI值。(3).  如果我们将FTI视为一个数据序列，则融合模块与递归神经网络中的更新过程非常相似[31]。fusion模块用于从2  toT展开的步骤。在展开的步骤t1（t=1，…，t−1） ，融合模块将步骤1处的特征F和FPN在步骤1处计算的特征F  1作为其输入。融合模块使用featureft 1，通过卷积层和Sigmoid操作计算注意力图。由此产生的注意力图用于计算时间DFT  1的加权和，以形成更新的FI。在以下步骤中，此函数将作为ft 1用于计算。在Sec的消融研究中。5，我们将展示RFP在有和没有融合模块的情况下的性能。

# 4. Switchable Atrous Convolution

## 4.1 Atrous Convolution

阿托拉斯卷积[12,32,57]是在任何卷积层放大滤波器视野的有效技术。特别是，使用atrus  rater引入atrus卷积−1连续过滤器值之间的零，等效地将ak×kfilter的内核大小扩大到ke=k（k−1） （r）−1）  不增加参数数量或计算量。图1b显示了一个3x3卷积层的示例，其衰减率设置为1（红色）和2（绿色）：使用不同衰减率，相同卷积权重集可以大致检测到不同比例的同一类对象。

## 4.2 Switchable Atrous Convolution

在本小节中，我们将详细介绍我们提出的可切换萎缩卷积（SAC）。图4显示了SAC的总体架构，它有三个主要组件：在SAC组件之前和之后附加的两个全局上下文模块。本节集中在中间的主要SAC组件，然后我们将解释全局上下文模块。

我们用y=Conv（x，w，r）表示以x为输入和输出的带权重和衰减率的卷积运算。然后，我们可以将卷积层转换为SAC，如下所示。

其中RHERE是SAC的超参数，∆wis是一个可训练权重，开关函数（·）被实现为一个平均池层，该层包含一个5x5内核，后跟一个1x1卷积层（见图4）。开关功能为输入和位置相关；因此，主干模型能够根据需要适应不同的规模。除非另有说明，我们在实验中设置r=3。

我们提出了一种锁定机制，将一个权重设置为asw，另一个权重设置为asw∆原因如下。对象检测器通常使用预先训练的检查点来初始化权重。然而，对于从标准卷积层转换而来的SAC层，较大萎缩率的权重缺失。由于不同比例的对象可以通过相同的权重和不同的衰减率粗略地检测到，因此很自然地使用预训练模型中的权重初始化缺失的权重。我们的实现使用sw∆w表示来自预训练检查站的缺失重量，以及∆wis已初始化为0。修理时∆w=0，我们观察到0的下降。1%AP。但是∆没有锁定机制的walone会大大降低AP的性能。

## 4.3 Global Context

如图4所示，我们在SAC的主要组件之前和之后插入了两个全局上下文模块。这两个模块是轻量级的，因为输入特征首先由全局平均池层压缩。全局上下文模块与SENet[33]相似，除了两个主要区别：（1）我们只有一个卷积层，没有任何非线性层；（2）输出被添加回主流，而不是将输入乘以由Sigmoid计算的重新校准值。实验上，我们发现在SAC组件之前添加全局上下文信息（即，向开关函数添加全局信息）对检测性能有积极影响。我们推测这是因为当全局信息可用时，可以做出更稳定的切换预测。然后，我们将全局信息移到开关函数之外，并将其放置在主体之前和之后，以便使con和scan都能从中受益。我们没有采用原始SENet公式，因为我们发现最终模型AP没有改进。在Sec的消融研究中。5，我们展示了SAC在有和没有全局上下文模块的情况下的性能。

## 4.4 Implementation Details

在我们的实现中，我们使用可变形卷积[19,92]来替换这两种卷积运算在等式4中。它们的偏移函数不是共享的，从预训练主干加载时初始化为predict0。第二节的实验。5将显示具有和不具有可变形卷积的SAC的性能比较。我们通过替换主干中的所有3x3卷积层，在ResNet及其变体[30,77]上采用SAC。全局上下文模块中的权重和偏差初始化为0。开关中的权重初始化为0，偏差设置为1。∆wis已初始化为0。上述初始化策略保证，当在ImageNet[63]上加载预训练的主干时，将所有3x3卷积层转换为SAC不会在COCO上进行任何训练步骤之前改变输出[51]。

# 5. Experiments

## 5.1 Experimental Details

我们在COCO数据集上进行了实验[51]。本文中提出的所有模型都是在Train2017的分割上训练的，该分割有115k标记图像。然后，我们在VAL2017和test-dev上测试模型。我们使用mmdetection实现检测器[9]。我们的基线模型是HTC[8]，它使用数据集中的边界框和实例分段注释。运行时间是在单个NVIDIA  TITAN  RTX图形卡上测量的。我们严格遵循HTC的实验设置[8]。对于消融研究，我们训练了12个时期的模型，学习率乘以0。1在8和12个时代之后。此外，其他培训和测试设置保持不变，不使用铃声和口哨。对于消融研究后的主要结果，我们使用多尺度训练，长边设置为1333，短边从[4001200]中随机取样。我们用学习率乘以0来训练40个时期的模型。第36和39个时代之后。软NMS[3]用于ResNeXt-101-32x4d和ResNeXt-101-64x4d。我们还报告了有无测试时间增加（TTA）的结果，包括水平翻转和多尺度测试，短边设置为[800、1000、1200、1400、1600]，长边设置为1。5倍短边。

## 5.2 Ablation Studies

在本小节中，我们将在Tab中展示RFP和SAC的消融研究。2和选项卡。3.标签。盒子和盒子是怎么回事 以ResNet-50和FPN为主干屏蔽基线HTC的AP。然后，我们将建议的RFP和SAC添加到基线HTC中，两者都能够将AP提高>4%，而不会使速度降低太多。将它们结合在一起，我们的探测器实现了49%的箱AP和42%。1%在3时屏蔽AP。每秒9帧。

标签。3展示了RFP和SAC的个体消融研究，其中我们介绍了其改进的来源。对于RFP，我们显示“RFP共享”，其中B1IANDB2共享其权重。我们还通过在“RFP-ASPP”和“RFP-fusion”中展示RFP的性能，展示了ASPP模块和fusion模块的改进。最后，我们将展开步骤从2增加到3，并得到“RFP  3X”，这进一步将长方体AP提高了1。3%.  对于SAC，我们首先对不含DCN的SAC进行了实验[19]（即“SAC-DCN”）。然后，我们证明了全局上下文能够在“SAC-DCN-global”中改进APSAC-DCN-锁定“断开图4中的锁定机制，其中第二卷积仅使用∆w、  证明重量锁定对SAC是必要的。最后，在“SAC-DCN  DS（双交换机）”中，我们替换了（x）和1−带两个独立开关S1（x）和S2（x）的S（x）。消融研究在Tab中进行。3表明RFP和SAC的配方在我们探索的设计空间内具有最佳配置。

图6显示了HTC、HTC RFP和HTC  SAC的结果。从这个比较中，我们注意到，RFP类似于选择性地增强或抑制神经元激活的人类视觉感知，能够更容易地找到被遮挡的对象，而附近的上下文信息对这些对象更为关键。由于SAC能够根据需要增加视野，因此它更能够检测图像中的大型物体。这也与表1中显示的SAC结果一致。2其APL较高的地方。图7显示了HTC、HTC  RFP、HTC SAC和检测器的训练损失。两者都能够显著加快培训过程，并收敛到较低的损失。

## 5.3 Main Results

在本小节中，我们展示了探测器的主要结果。我们为最先进的探测器HTC配备了探测器，并使用ResNet-50和ResNeXt-101作为探测器的主干。边界框检测结果显示在选项卡中。4.结果分为4组。第一组显示单级探测器。第二组显示多级探测器。第三组是HTC，它是探测器的基线。第四组是我们的结果。结果也可以分为简单测试结果和TTA结果，其中TTA是测试时间增加的缩写。第三列显示是否使用TTA。注意，不同的方法使用不同的TTA策略。例如，CBNet使用了一种强大的TTA策略，可以将其box  AP从50提高到50。百分之七至五十三。3%.  我们的TTA战略只带来1。使用ResNeXt-101-32x4d作为主干时，性能提高4%。不同探测器之间的简单测试设置也可能存在显著差异。探测器使用（1333800）作为测试图像大小。较大的输入大小往往会带来改进（见[70]）。探测器采用与HTC相同的设置。

我们还将在选项卡中显示实例分割结果。5.选项卡中的方法尽可能多。4在他们的论文中没有提供掩模AP，我们只是将探测器与其基线HTC进行比较。边界框和遮罩对象检测的实验设置相同，只是我们报告了APbbox的APmaskinstead。从选项卡。5，我们可以看到，与边界框结果一致，检测器也带来了对其基线的重大改进，例如分割。

最后，全景分割结果显示在选项卡中。6.由于探测器只检测物体，我们使用DeepLabv3[15]和主干WideResNet-41[11,76,84]的物体预测。使用Panopics  API[37]中提供的脚本，在不调整任何超参数的情况下，将东西和东西的预测结合起来，我们设置了一个新的

## 5.4 Visualizing Learned Switches

图8显示了选项卡中“SAC-DCN”最后一个开关功能输出的可视化结果。3.图中较暗的强度意味着该区域的开关功能从较大的衰减率收集更多的输出。将开关输出与原始图像进行比较，我们观察到开关输出与地面真值对象比例很好地对齐。这些结果证明了可切换阿托斯卷积的行为与我们的直觉一致，当遇到大型物体时，我们的直觉倾向于使用更大的阿托斯速率。

# 6. Conclusion

在本文中，受“三思而后行”的设计思想的启发，我们提出了检测器，包括递归特征金字塔和可切换的阿托斯卷积。递归特征金字塔在宏观层面实现了两次思考，其中FPN的输出通过反馈连接返回到自底向上主干的每个阶段。可切换阿托斯卷积在微观层面上进行了两次实例化，其中输入以两种不同的阿托斯速率卷积。检测器在COCO上测试，用于目标检测、实例分割和全景分割。它为所有这些任务设定了最先进的结果。